{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiGFf5AxNL5T"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from matplotlib import pyplot\n",
        "from numpy import interp\n",
        "import sklearn, tensorflow\n",
        "import xlsxwriter\n",
        "import xlrd\n",
        "from sklearn import svm #, grid_search\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import metrics\n",
        "from sklearn import model_selection\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import gzip\n",
        "import pandas as pd\n",
        "import pdb\n",
        "import random\n",
        "from random import randint\n",
        "import scipy.io\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_dn1SHSHcGV",
        "outputId": "897aa384-9b7d-45c8-9fc2-c4377468610f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.1.9-py3-none-any.whl (154 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m122.9/154.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.1.9\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install xlsxwriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqPXuUNFUieQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential, model_from_config,Model\n",
        "from tensorflow.keras.layers import  Dropout, Activation, Flatten,LayerNormalization, PReLU#, Merge\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import SGD, RMSprop, Adadelta, Adagrad, Adam\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.constraints import MaxNorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xilJ9uPBYQEN",
        "outputId": "33d1d7e6-6639-4b16-da8c-c37df625f7f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.2)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow)\n",
            "  Downloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.16,>=2.15.0 (from tensorflow)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, keras, tensorboard, tensorflow\n",
            "\u001b[33m  WARNING: The script tensorboard is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The scripts estimator_ckpt_converter, import_pb_to_tensorboard, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 tensorboard-2.15.1 tensorflow-2.15.0 tensorflow-estimator-2.15.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install tensorflow --upgrade --user\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_GOj9ZZYT5a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nm1d_iS8U_uq",
        "outputId": "043ed069-4bc7-4c35-addf-c8e255fc335b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: tensorflow 2.14.0\n",
            "Uninstalling tensorflow-2.14.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.10/dist-packages/tensorflow-2.14.0.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/tensorflow/*\n",
            "Proceed (Y/n)? y\n",
            "Y\n",
            "  Successfully uninstalled tensorflow-2.14.0\n"
          ]
        }
      ],
      "source": [
        "pip uninstall tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgnoFVnDT7o4",
        "outputId": "9fe0eb25-6a58-4e6d-d670-050621d369ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras) (1.23.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.10/dist-packages (from keras) (1.11.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from keras) (1.16.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from keras) (6.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from keras) (1.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Wa0HDbjITOBf",
        "outputId": "e94b3216-69a3-4aa2-b492-c3fd5853dd44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.2)\n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/tensorboard/\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting tensorboard<2.16,>=2.15 (from tensorflow)\n",
            "  Downloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.16,>=2.15.0 (from tensorflow)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.14.0\n",
            "    Uninstalling tensorflow-estimator-2.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.14.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: Keras 2.3.1\n",
            "    Uninstalling Keras-2.3.1:\n",
            "      Successfully uninstalled Keras-2.3.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.14.1\n",
            "    Uninstalling tensorboard-2.14.1:\n",
            "      Successfully uninstalled tensorboard-2.14.1\n",
            "Successfully installed keras-2.15.0 tensorboard-2.15.1 tensorflow-2.15.0 tensorflow-estimator-2.15.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras",
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fi8-6cE1TAYq",
        "outputId": "c35ad21b-ac69-4cc3-ec44-17e564800e0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.1.9-py3-none-any.whl (154 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/154.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.1.9\n"
          ]
        }
      ],
      "source": [
        "!pip install xlsxwriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIm0Tx2oR2d8"
      },
      "outputs": [],
      "source": [
        "def prepare_data(seperate=False):\n",
        "    print (\"loading data\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    disease_fea = np.loadtxt(\"integrated disease similarity.txt\",dtype=float,delimiter=\"\\t\")\n",
        "    miRNA_fea  = np.loadtxt(\"integrated circRNA similarity.txt\",dtype=float,delimiter=\"\\t\")\n",
        "    #miRNA_fea  = np.loadtxt(\"CC.txt\",dtype=float,delimiter=\",\")  #Gauss only\n",
        "    interaction = np.loadtxt(\"Association Matrix.txt\",dtype=int,delimiter=\"\\t\")\n",
        "\n",
        "    # pca=PCA(n_components=219)\n",
        "    # x=pca.fit_transform(miRNA_fea,y=219)\n",
        "    # miRNA_fea=x\n",
        "    # print(miRNA_fea.shape)\n",
        "\n",
        "    # pca=PCA(n_components=34)\n",
        "    # x=pca.fit_transform(disease_fea,y=34)\n",
        "    # disease_fea=x\n",
        "    # print(disease_fea.shape)\n",
        "\n",
        "\n",
        "    link_number = 0\n",
        "    #nonlink_number=0\n",
        "    train = []\n",
        "    testfnl= []\n",
        "    label1 = []\n",
        "    label2 = []\n",
        "    label22=[]\n",
        "    ttfnl=[]\n",
        "    #link_position = []\n",
        "    #nonLinksPosition = []\n",
        "\n",
        "    for i in range(0, interaction.shape[0]):   # shape[0] returns m if interaction is m*n, ie, returns no. of rows of matrix\n",
        "        for j in range(0, interaction.shape[1]):\n",
        "\n",
        "            if interaction[i, j] == 1:                      #for associated\n",
        "                label1.append(interaction[i,j])             #label1= labels for association(1)\n",
        "                link_number = link_number + 1               #no. of associated samples\n",
        "                #link_position.append([i, j])\n",
        "                miRNA_fea_tmp = list(miRNA_fea[i])\n",
        "                disease_fea_tmp = list(disease_fea[j])\n",
        "                tmp_fea = (miRNA_fea_tmp,disease_fea_tmp)   #concatnated feature vector for an association\n",
        "                train.append(tmp_fea)                       #train contains feature vectors of all associated samples\n",
        "            elif interaction[i,j] == 0:                     #for no association\n",
        "                label2.append(interaction[i,j])             #label2= labels for no association(0)\n",
        "                #nonlink_number = nonlink_number + 1\n",
        "                #nonLinksPosition.append([i, j])\n",
        "                miRNA_fea_tmp1 = list(miRNA_fea[i])\n",
        "                disease_fea_tmp1 = list(disease_fea[j])\n",
        "                test_fea= (miRNA_fea_tmp1,disease_fea_tmp1) #concatenated feature vector for not having association\n",
        "                testfnl.append(test_fea)                    #testfnl contains feature vectors of all non associated samples\n",
        "    #print(train)\n",
        "    #print('************')\n",
        "    #print(testfnl)\n",
        "    #print(link_number)\n",
        "    #m = np.arange(2000)  #half size negative data\n",
        "\n",
        "    print(\"link_number\",link_number)\n",
        "\n",
        "    m = np.arange(len(label2))\n",
        "    np.random.shuffle(m)\n",
        "\n",
        "    for x in m:\n",
        "        ttfnl.append(testfnl[x])\n",
        "        label22.append(label2[x])\n",
        "    #print('************')\n",
        "    #print(ttfnl)\n",
        "    #print('************')\n",
        "    #print(label22)\n",
        "    for x in range(0, link_number):                         #for equalizing positive and negative samples\n",
        "        tfnl= ttfnl[x]                                    #tfnl= feature vector pair for no association\n",
        "        lab= label22[x]                                      #lab= label of the above mentioned feature vector pair(0)\n",
        "        #print(tfnl)\n",
        "        #print('***')\n",
        "        train.append(tfnl)                                  #append the non associated feature vector pairs to train till x<=no. of associated pairs\n",
        "        label1.append(lab)                                   #append the labels of non associated pairs(0) to label1\n",
        "\n",
        "    #print(train)\n",
        "    #print(label1)\n",
        "    return np.array(train), label1, np.array(testfnl)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ew_iuX_sUp9O"
      },
      "outputs": [],
      "source": [
        " from sklearn.preprocessing import LabelEncoder\n",
        "def calculate_performace(test_num, pred_y,  labels): #pred_y = proba, labels = real_labels\n",
        "    tp =0\n",
        "    fp = 0\n",
        "    tn = 0\n",
        "    fn = 0\n",
        "    for index in range(test_num):\n",
        "        if labels[index] ==1:\n",
        "            if labels[index] == pred_y[index]:\n",
        "                tp = tp +1\n",
        "            else:\n",
        "                fn = fn + 1\n",
        "        else:\n",
        "            if labels[index] == pred_y[index]:\n",
        "                tn = tn +1\n",
        "            else:\n",
        "                fp = fp + 1\n",
        "\n",
        "    acc = float(tp + tn)/test_num\n",
        "\n",
        "    if tp == 0 and fp == 0:\n",
        "        precision = 0\n",
        "        MCC = 0\n",
        "        f1_score=0\n",
        "        sensitivity =  float(tp)/ (tp+fn)\n",
        "        specificity = float(tn)/(tn + fp)\n",
        "    else:\n",
        "        precision = float(tp)/(tp+ fp)\n",
        "        sensitivity = float(tp)/ (tp+fn)\n",
        "        specificity = float(tn)/(tn + fp)\n",
        "        MCC = float(tp*tn-fp*fn)/(np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\n",
        "        f1_score= float(2*tp)/((2*tp)+fp+fn)\n",
        "\n",
        "    return acc, precision, sensitivity, specificity, MCC,f1_score\n",
        "\n",
        "def transfer_array_format(data):    #data=X  , X= all the miRNA features, disease features\n",
        "    formated_matrix1 = []\n",
        "    formated_matrix2 = []\n",
        "    #pdb.set_trace()\n",
        "    #pdb.set_trace()\n",
        "    for val in data:\n",
        "        #formated_matrix1.append(np.array([val[0]]))\n",
        "        formated_matrix1.append(val[0])   #contains miRNA features ?\n",
        "        formated_matrix2.append(val[1])   #contains disease features ?\n",
        "        #formated_matrix1[0] = np.array([val[0]])\n",
        "        #formated_matrix2.append(np.array([val[1]]))\n",
        "        #formated_matrix2[0] = val[1]\n",
        "\n",
        "    return np.array(formated_matrix1), np.array(formated_matrix2)\n",
        "\n",
        "def preprocess_labels(labels, encoder=None, categorical=True):\n",
        "    if not encoder:\n",
        "        encoder = LabelEncoder()\n",
        "        encoder.fit(labels)\n",
        "    y = encoder.transform(labels).astype(np.int32)\n",
        "    if categorical:\n",
        "        y =to_categorical(y)\n",
        "    return y, encoder\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcWGRjSMVpKd"
      },
      "outputs": [],
      "source": [
        "\n",
        "def DNN():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(input_dim=128, output_dim=500))#, 1371,878 shapeinit='glorot_normal')) ## 1027 1261 1021 918 128 878 638 535\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Dense(input_dim=500, output_dim=500,init='glorot_normal'))  ##500  # Hidden layer1\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Dense(input_dim=500, output_dim=300))#,init='glorot_normal'))  ##500 # Hidden layer 2\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Dense(input_dim=300, output_dim=2))#,init='glorot_normal'))  ##500  #  Output layer\n",
        "    model.add(Activation('sigmoid'))\n",
        "    #sgd = SGD(l2=0.0,lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    adadelta = Adadelta(lr=1.0, rho=0.95, epsilon=1e-08)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=adadelta) #, class_mode=\"binary\")##rmsprop sgd\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRJOve5LSQFk"
      },
      "outputs": [],
      "source": [
        "\n",
        "def DNN_auto(x_train):\n",
        "\n",
        "    encoding_dim = 128 #128 original\n",
        "    input_img = Input(shape=(673,))\n",
        "\n",
        "\n",
        "    encoded = Dense(500, activation='relu')(input_img)   # 450 - output (input layer)\n",
        "    #encoded = Dense(250, activation='relu')(encoded)     # 200 - output (hidden layer1)\n",
        "    encoded = Dense(128, activation='relu')(encoded)     # 100 - output (hidden layer2)\n",
        "    encoder_output = Dense(encoding_dim)(encoded)        # 128 - output (encoding layer)\n",
        "    print()\n",
        "# decoder layers\n",
        "    decoded = Dense(128, activation='relu')(encoder_output)\n",
        "    #decoded = Dense(250, activation='relu')(decoded)\n",
        "    decoded = Dense(500, activation='relu')(decoded)\n",
        "    decoded = Dense(673, activation='tanh')(decoded)\n",
        "\n",
        "    # autoencoder = Model(input=input_img, output=decoded)\n",
        "\n",
        "    # encoder = Model(input=input_img, output=encoder_output)\n",
        "\n",
        "    autoencoder = Model(input_img, decoded) #just remove input=, output= in new tensorflow update\n",
        "\n",
        "    encoder = Model(input_img,encoder_output)\n",
        "\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "\n",
        "    autoencoder.fit(x_train, x_train,epochs=20,batch_size=100,shuffle=True)  # second x_train is given instead of train labels in DNN, ie here, i/p=o/p\n",
        "\n",
        "    #batch_size=100 original\n",
        "    encoded_imgs = encoder.predict(x_train)\n",
        "\n",
        "    print(\"???????????????????\")\n",
        "    print(encoded_imgs.shape)  #(910,128)\n",
        "\n",
        "    return encoder_output,encoded_imgs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "id": "T___B9J3SYEZ",
        "outputId": "2409b639-511c-485c-c4bc-151f319714bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading data\n",
            "link_number 650\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-5130a23dcc33>:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return np.array(train), label1, np.array(testfnl)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "************\n",
            "(1300, 585) (1300, 88)\n",
            "******************\n",
            "************\n",
            "(1300, 673)\n",
            "******************\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-b955c0a6589e>\u001b[0m in \u001b[0;36m<cell line: 215>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mDeepMDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-b955c0a6589e>\u001b[0m in \u001b[0;36mDeepMDA\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# labels labels_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#num gets an array like num = [0,1,2...len(y)], len(y) = 512*71 = 36352\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-983cdecec5c9>\u001b[0m in \u001b[0;36mpreprocess_labels\u001b[0;34m(labels, encoder, categorical)\u001b[0m\n\u001b[1;32m     55\u001b[0m    \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mcategorical\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m        \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np_utils' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def DeepMDA():\n",
        "    X, labels,T = prepare_data(seperate = True)     #X= array of concatinated features,labels=corresponding labels\n",
        "    #import pdb            #python debugger\n",
        "\n",
        "    X_data1, X_data2 = transfer_array_format(X) # X-data1 = miRNA features(2500*495),  X_data2 = disease features (2500*383)\n",
        "\n",
        "    print(\"************\")\n",
        "    print (X_data1.shape,X_data2.shape)  # (36352,512), (36352,71)\n",
        "    print(\"******************\")\n",
        "\n",
        "\n",
        "    X_data1= np.concatenate((X_data1, X_data2 ), axis = 1) #axis=1 , rowwise concatenation\n",
        "\n",
        "    print(\"************\")\n",
        "    print (X_data1.shape)  # (36352,583)\n",
        "    print(\"******************\")\n",
        "\n",
        "\n",
        "    y, encoder = preprocess_labels(labels)# labels labels_new\n",
        "    num = np.arange(len(y))   #num gets an array like num = [0,1,2...len(y)], len(y) = 512*71 = 36352\n",
        "    np.random.shuffle(num)\n",
        "    X_data1 = X_data1[num]\n",
        "    X_data2 = X_data2[num]\n",
        "    y = y[num]\n",
        "\n",
        "    t=0\n",
        "    mean_tpr = 0.0\n",
        "    mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "\n",
        "\n",
        "    encoder,X_data1 = DNN_auto(X_data1)             # Now X_data1 gets Auto encoded output\n",
        "\n",
        "    #encoder,X_data2 = DNN_auto(X_data2)\n",
        "\n",
        "\n",
        "\n",
        "    num_cross_val = 5\n",
        "    all_performance = []\n",
        "    all_performance_rf = []\n",
        "    all_performance_bef = []\n",
        "    all_performance_DNN = []\n",
        "    all_performance_SDADNN = []\n",
        "    all_performance_blend = []\n",
        "    all_labels = []\n",
        "    all_prob = {}\n",
        "    num_classifier = 3\n",
        "    all_prob[0] = []\n",
        "    all_prob[1] = []\n",
        "    all_prob[2] = []\n",
        "    all_prob[3] = []\n",
        "    all_averrage = []\n",
        "    for fold in range(num_cross_val):\n",
        "        train1 = np.array([x for i, x in enumerate(X_data1) if i % num_cross_val != fold])\n",
        "        test1 = np.array([x for i, x in enumerate(X_data1) if i % num_cross_val == fold])\n",
        "        #train2 = np.array([x for i, x in enumerate(X_data2) if i % num_cross_val != fold])\n",
        "        #test2 = np.array([x for i, x in enumerate(X_data2) if i % num_cross_val == fold])\n",
        "        train_label = np.array([x for i, x in enumerate(y) if i % num_cross_val != fold])\n",
        "        test_label = np.array([x for i, x in enumerate(y) if i % num_cross_val == fold])\n",
        "        #print(\"$$$$$$$$$$$$\",test1)\n",
        "        #print(test2)\n",
        "\n",
        "        real_labels = []\n",
        "        for val in test_label:\n",
        "            if val[0] == 1:             #tuples in array, val[0]- first element of tuple\n",
        "                real_labels.append(0)\n",
        "            else:\n",
        "                real_labels.append(1)\n",
        "\n",
        "        train_label_new = []\n",
        "        for val in train_label:\n",
        "            if val[0] == 1:\n",
        "                train_label_new.append(0)\n",
        "            else:\n",
        "                train_label_new.append(1)\n",
        "        class_index = 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "       # prefilter_train_bef, prefilter_test_bef = autoencoder_two_subnetwork_fine_tuning(train1, train2, train_label, test1, test2, test_label)\n",
        "\n",
        "\n",
        "\n",
        "        ## DNN\n",
        "        class_index = class_index + 1\n",
        "#        prefilter_train = np.concatenate((train1, train2), axis = 1)\n",
        "#        prefilter_test = np.concatenate((test1, test2), axis = 1)\n",
        "\n",
        "        prefilter_train = train1\n",
        "        prefilter_test = test1\n",
        "#\n",
        "       # model_DNN = DNN()\n",
        "\n",
        "        #encoder,encoder_imgs=DNN_auto(prefilter_train)\n",
        "\n",
        "\n",
        "     #   clf = svm.SVC(kernel='linear')\n",
        "   #     clf = svm.SVC(kernel='rbf')          #Radial basis function as kernel\n",
        "#        clf = svm.SVC(kernel='sigmoid')\n",
        "        #clf = svm.SVC(kernel='poly')\n",
        "        #clf = svm.SVC(kernel='poly',degree=3,gamma=2)\n",
        "\n",
        "        #clf = svm.SVC(kernel='poly',degree=3,gamma=2,probability=True)\n",
        "        #clf = svm.SVC(kernel='rbf',degree=3,gamma=2)  #infinite\n",
        "\n",
        "        #clf = RandomForestClassifier(n_estimators=100)\n",
        "        clf = XGBClassifier(n_estimators = 250, learning_rate = 0.2, max_depth= 15)\n",
        "\n",
        "        clf.fit(prefilter_train, train_label_new)    #***Training\n",
        "\n",
        "        ae_y_pred_prob = clf.predict_proba(prefilter_test)[:,1]   #**testing\n",
        "\n",
        "        #print(ae_y_pred_prob)\n",
        "\n",
        "        proba = transfer_label_from_prob(ae_y_pred_prob)\n",
        "        #print(proba)\n",
        "\n",
        "\n",
        "        #print(encoder.shape)\n",
        "        print(X_data1.shape)\n",
        "        #print(X_data2.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "         #**********************************\n",
        "        #Writing result to excel sheet named RESULT\n",
        "        #********************************\n",
        "#\n",
        "#\n",
        "#\n",
        "         #**************Finished writing to excel\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        acc, precision, sensitivity, specificity, MCC, f1_score = calculate_performace(len(real_labels), proba,  real_labels)\n",
        "\n",
        "        fpr, tpr, auc_thresholds = roc_curve(real_labels, ae_y_pred_prob)\n",
        "        auc_score = auc(fpr, tpr)\n",
        "\n",
        "#        print(\"Length\")\n",
        "#        print(len(real_labels))\n",
        "\n",
        "        scipy.io.savemat('raw_DNN',{'fpr':fpr,'tpr':tpr,'auc_score':auc_score})\n",
        "\n",
        "\n",
        "        ## AUPR score add\n",
        "        precision1, recall, pr_threshods = precision_recall_curve(real_labels, ae_y_pred_prob)\n",
        "        aupr_score = auc(recall, precision1)\n",
        "        print (\"AUTO-RF:\",acc, precision, sensitivity, specificity, MCC, auc_score, aupr_score,f1_score)\n",
        "        all_performance_DNN.append([acc, precision, sensitivity, specificity, MCC, auc_score, aupr_score,f1_score])\n",
        "        t =t+1  #  AUC fold number\n",
        "\n",
        "        pyplot.plot(fpr,tpr,label= 'ROC fold %d (AUC = %0.4f)' % (t, auc_score))\n",
        "        mean_tpr += interp(mean_fpr, fpr, tpr) # one dimensional interpolation\n",
        "        mean_tpr[0] = 0.0\n",
        "\n",
        "\n",
        "\n",
        "        pyplot.xlabel('False positive rate, (1-Specificity)')\n",
        "        pyplot.ylabel('True positive rate,(Sensitivity)')\n",
        "        pyplot.title('Receiver Operating Characteristic curve: 5-Fold CV')\n",
        "        pyplot.legend()\n",
        "\n",
        "    mean_tpr /= num_cross_val\n",
        "    mean_tpr[-1] = 1.0\n",
        "    mean_auc = auc(mean_fpr, mean_tpr)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    pyplot.plot(mean_fpr, mean_tpr,'--' ,linewidth=2.5,label='Mean ROC (AUC = %0.4f)' % mean_auc)\n",
        "    pyplot.legend()\n",
        "\n",
        "\n",
        "    pyplot.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print('*******AUTO-RF*****')\n",
        "    print ('mean performance of rf using raw feature')\n",
        "    print (np.mean(np.array(all_performance_DNN), axis=0))\n",
        "    Mean_Result=[]\n",
        "    Mean_Result= np.mean(np.array(all_performance_DNN), axis=0)\n",
        "    print ('---' * 20)\n",
        "    print('Mean-Accuracy=', Mean_Result[0],'\\n Mean-precision=',Mean_Result[1])\n",
        "    print('Mean-Sensitivity=', Mean_Result[2], '\\n Mean-Specificity=',Mean_Result[3])\n",
        "    print('Mean-MCC=', Mean_Result[4],'\\n' 'Mean-auc_score=',Mean_Result[5])\n",
        "    print('Mean-Aupr-score=', Mean_Result[6],'\\n' 'Mean_F1=',Mean_Result[7])\n",
        "    print ('---' * 20)\n",
        "\n",
        "    print(X_data1.shape)\n",
        "\n",
        "\n",
        "\n",
        "    #print(len(real_labels))     ## = 7168\n",
        "    #print(i,j)\n",
        "\n",
        "    #print (X_data1.shape,X_data2.shape)  #X_data1= 2500*495, X_data2=2500*383\n",
        "    #print(X.shape)  # X = 2500*2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def transfer_label_from_prob(proba):\n",
        "    label = [1 if val>=0.5 else 0 for val in proba]\n",
        "    return label\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    DeepMDA()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}